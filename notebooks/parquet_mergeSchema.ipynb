{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7c33c22-c5b0-470e-b2a0-eed5bcc3f9cf",
   "metadata": {},
   "source": [
    "# mergeSchema example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d24784f8-50a2-4eec-b244-e269439606e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9201694-c662-4abd-be1b-ef0ad90f3c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/16 18:53:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"mrpowers\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d559361f-6e16-4cd8-b28e-21e186b769ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(1, \"a\"), (2, \"b\")], [\"col_a\", \"col_b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0190cc9-df44-4675-91a5-02ce6b74e489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|col_a|col_b|\n",
      "+-----+-----+\n",
      "|    1|    a|\n",
      "|    2|    b|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af542b17-ad3a-4cc9-b198-f27c7e869dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.format(\"parquet\").save(\"tmp/parquet_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d499ca5e-e9d6-4d1a-91dc-a3154d0bec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(\"x\", \"fun\"), (\"y\", \"bye\")], [\"col_b\", \"col_c\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e61435e0-a9fa-4715-b2e8-90b6b04a06d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"parquet\").mode(\"append\").save(\"tmp/parquet_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744e56c8-1da5-4ece-8fec-eca3ec1ed492",
   "metadata": {},
   "source": [
    "## \"Regular read\" doesn't give you desired result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11286802-e306-4040-851e-2a5bf1b16029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|col_a|col_b|\n",
      "+-----+-----+\n",
      "|    1|    a|\n",
      "|    2|    b|\n",
      "| null|    x|\n",
      "| null|    y|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"parquet\").load(\"tmp/parquet_table\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea72c0f-7cc5-4c54-9f99-4a6ca72f288e",
   "metadata": {},
   "source": [
    "## You have to set mergeSchema to properly read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbf87c13-2509-40f0-a277-7593d1736102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|col_a|col_b|col_c|\n",
      "+-----+-----+-----+\n",
      "|    1|    a| null|\n",
      "|    2|    b| null|\n",
      "| null|    x|  fun|\n",
      "| null|    y|  bye|\n",
      "+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"parquet\").option(\"mergeSchema\", True).load(\n",
    "    \"tmp/parquet_table\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc04ab80-c373-492b-a628-2737ec33616a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark-330-delta-210] *",
   "language": "python",
   "name": "conda-env-pyspark-330-delta-210-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
