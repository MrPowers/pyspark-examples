{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b20aeebc-7b74-4483-bcc8-65d4e7248f39",
   "metadata": {},
   "source": [
    "# PySpark vs other engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d698c57-8f8f-42ad-869e-3148b928833d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import delta\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from delta import configure_spark_with_delta_pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb18aa7a-b0cc-4563-93f0-7d6d266b3de5",
   "metadata": {},
   "source": [
    "## PySpark groupby query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af136861-d16d-4274-a4a9-219ce9dffc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/matthew.powers/opt/miniconda3/envs/pyspark-340-delta-240/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/matthew.powers/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/matthew.powers/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-144bd826-4b9a-414f-8429-9dad3114a77a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 106ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-144bd826-4b9a-414f-8429-9dad3114a77a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/4ms)\n",
      "23/10/27 18:11:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "builder = (\n",
    "    pyspark.sql.SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.executor.memory\", \"10G\")\n",
    "    .config(\"spark.driver.memory\", \"25G\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "213e069c-1d8f-4172-9629-e227d1492332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:===================================================>  (180 + 7) / 187]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25 ms, sys: 9.94 ms, total: 34.9 ms\n",
      "Wall time: 7.68 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id1='id016', sum(v1)=30003304),\n",
       " Row(id1='id074', sum(v1)=30006309),\n",
       " Row(id1='id070', sum(v1)=29990210),\n",
       " Row(id1='id054', sum(v1)=30011978),\n",
       " Row(id1='id053', sum(v1)=29992360),\n",
       " Row(id1='id056', sum(v1)=29987234),\n",
       " Row(id1='id057', sum(v1)=29991822),\n",
       " Row(id1='id003', sum(v1)=30003365),\n",
       " Row(id1='id001', sum(v1)=30009448),\n",
       " Row(id1='id015', sum(v1)=30006177),\n",
       " Row(id1='id017', sum(v1)=29995061),\n",
       " Row(id1='id018', sum(v1)=29992469),\n",
       " Row(id1='id014', sum(v1)=29998476),\n",
       " Row(id1='id072', sum(v1)=30003522),\n",
       " Row(id1='id071', sum(v1)=29998357),\n",
       " Row(id1='id073', sum(v1)=30006820),\n",
       " Row(id1='id055', sum(v1)=30009993),\n",
       " Row(id1='id004', sum(v1)=30015990),\n",
       " Row(id1='id002', sum(v1)=29996534),\n",
       " Row(id1='id005', sum(v1)=29993888),\n",
       " Row(id1='id050', sum(v1)=30008271),\n",
       " Row(id1='id052', sum(v1)=30014118),\n",
       " Row(id1='id021', sum(v1)=29982118),\n",
       " Row(id1='id041', sum(v1)=29994657),\n",
       " Row(id1='id040', sum(v1)=29989173),\n",
       " Row(id1='id049', sum(v1)=29978475),\n",
       " Row(id1='id051', sum(v1)=29994785),\n",
       " Row(id1='id020', sum(v1)=29993667),\n",
       " Row(id1='id019', sum(v1)=29998785),\n",
       " Row(id1='id022', sum(v1)=29994847),\n",
       " Row(id1='id043', sum(v1)=30005705),\n",
       " Row(id1='id042', sum(v1)=29989540),\n",
       " Row(id1='id044', sum(v1)=29999327),\n",
       " Row(id1='id059', sum(v1)=30010798),\n",
       " Row(id1='id058', sum(v1)=29999957),\n",
       " Row(id1='id077', sum(v1)=29990807),\n",
       " Row(id1='id076', sum(v1)=29998420),\n",
       " Row(id1='id078', sum(v1)=29998434),\n",
       " Row(id1='id097', sum(v1)=30015928),\n",
       " Row(id1='id098', sum(v1)=29997789),\n",
       " Row(id1='id061', sum(v1)=30012298),\n",
       " Row(id1='id060', sum(v1)=30021845),\n",
       " Row(id1='id075', sum(v1)=30013372),\n",
       " Row(id1='id100', sum(v1)=29987827),\n",
       " Row(id1='id099', sum(v1)=30009485),\n",
       " Row(id1='id096', sum(v1)=29993372),\n",
       " Row(id1='id093', sum(v1)=29988829),\n",
       " Row(id1='id092', sum(v1)=29996666),\n",
       " Row(id1='id046', sum(v1)=30010825),\n",
       " Row(id1='id048', sum(v1)=29998849),\n",
       " Row(id1='id094', sum(v1)=30005130),\n",
       " Row(id1='id095', sum(v1)=30010887),\n",
       " Row(id1='id045', sum(v1)=29992441),\n",
       " Row(id1='id047', sum(v1)=29972409),\n",
       " Row(id1='id037', sum(v1)=29996759),\n",
       " Row(id1='id036', sum(v1)=29994349),\n",
       " Row(id1='id025', sum(v1)=30016745),\n",
       " Row(id1='id024', sum(v1)=30003956),\n",
       " Row(id1='id084', sum(v1)=30005578),\n",
       " Row(id1='id086', sum(v1)=30003608),\n",
       " Row(id1='id083', sum(v1)=30005209),\n",
       " Row(id1='id038', sum(v1)=29995309),\n",
       " Row(id1='id039', sum(v1)=30000168),\n",
       " Row(id1='id026', sum(v1)=29993858),\n",
       " Row(id1='id023', sum(v1)=29988818),\n",
       " Row(id1='id087', sum(v1)=29997379),\n",
       " Row(id1='id085', sum(v1)=30010513),\n",
       " Row(id1='id006', sum(v1)=30006882),\n",
       " Row(id1='id065', sum(v1)=30007777),\n",
       " Row(id1='id033', sum(v1)=29983262),\n",
       " Row(id1='id007', sum(v1)=29992448),\n",
       " Row(id1='id008', sum(v1)=29999024),\n",
       " Row(id1='id009', sum(v1)=29994474),\n",
       " Row(id1='id063', sum(v1)=30006173),\n",
       " Row(id1='id064', sum(v1)=29985828),\n",
       " Row(id1='id062', sum(v1)=29996661),\n",
       " Row(id1='id034', sum(v1)=30010786),\n",
       " Row(id1='id032', sum(v1)=29986434),\n",
       " Row(id1='id035', sum(v1)=30003917),\n",
       " Row(id1='id031', sum(v1)=29998489),\n",
       " Row(id1='id080', sum(v1)=29979880),\n",
       " Row(id1='id082', sum(v1)=30007351),\n",
       " Row(id1='id090', sum(v1)=29994958),\n",
       " Row(id1='id088', sum(v1)=29999642),\n",
       " Row(id1='id089', sum(v1)=29990077),\n",
       " Row(id1='id081', sum(v1)=29988686),\n",
       " Row(id1='id079', sum(v1)=30009124),\n",
       " Row(id1='id091', sum(v1)=29995955),\n",
       " Row(id1='id068', sum(v1)=30008470),\n",
       " Row(id1='id066', sum(v1)=29997843),\n",
       " Row(id1='id013', sum(v1)=29989026),\n",
       " Row(id1='id010', sum(v1)=30006196),\n",
       " Row(id1='id029', sum(v1)=30003726),\n",
       " Row(id1='id028', sum(v1)=29988103),\n",
       " Row(id1='id069', sum(v1)=30017471),\n",
       " Row(id1='id067', sum(v1)=29996312),\n",
       " Row(id1='id012', sum(v1)=29990141),\n",
       " Row(id1='id011', sum(v1)=30002510),\n",
       " Row(id1='id027', sum(v1)=30000498),\n",
       " Row(id1='id030', sum(v1)=29991188)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/27 18:13:14 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /private/var/folders/19/_52w4zps3xjc6plz_f63j8sh0000gp/T/blockmgr-f2aa96a0-a934-46ff-9f90-b94d5646b5ef. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /private/var/folders/19/_52w4zps3xjc6plz_f63j8sh0000gp/T/blockmgr-f2aa96a0-a934-46ff-9f90-b94d5646b5ef\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:177)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:113)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2052)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2175)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2175)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2081)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$31(SparkContext.scala:664)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/10/27 18:13:14 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /private/var/folders/19/_52w4zps3xjc6plz_f63j8sh0000gp/T/blockmgr-f2aa96a0-a934-46ff-9f90-b94d5646b5ef/05. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /private/var/folders/19/_52w4zps3xjc6plz_f63j8sh0000gp/T/blockmgr-f2aa96a0-a934-46ff-9f90-b94d5646b5ef/05\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:177)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:113)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2052)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2175)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2175)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2081)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$31(SparkContext.scala:664)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/10/27 18:13:15 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /private/var/folders/19/_52w4zps3xjc6plz_f63j8sh0000gp/T/blockmgr-f2aa96a0-a934-46ff-9f90-b94d5646b5ef/18. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /private/var/folders/19/_52w4zps3xjc6plz_f63j8sh0000gp/T/blockmgr-f2aa96a0-a934-46ff-9f90-b94d5646b5ef/18\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:177)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:113)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2052)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2175)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2175)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2081)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$31(SparkContext.scala:664)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark.read.format(\"delta\").load(f\"{Path.home()}/data/deltalake/G1_1e9_1e2_0_0\").groupBy(\n",
    "    \"id1\"\n",
    ").sum(\"v1\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32971851-2666-404e-b6a5-a0322466ed64",
   "metadata": {},
   "source": [
    "## Polars groupby query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f36cd7f2-c0fc-467d-9b50-8f35ed15d9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49e57c8e-c8fd-42c5-b38c-db377ea6c9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.8 s, sys: 10.4 s, total: 40.3 s\n",
      "Wall time: 12.4 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (100, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id1</th><th>v1_sum</th></tr><tr><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;id054&quot;</td><td>30011978</td></tr><tr><td>&quot;id055&quot;</td><td>30009993</td></tr><tr><td>&quot;id042&quot;</td><td>29989540</td></tr><tr><td>&quot;id084&quot;</td><td>30005578</td></tr><tr><td>&quot;id083&quot;</td><td>30005209</td></tr><tr><td>&quot;id075&quot;</td><td>30013372</td></tr><tr><td>&quot;id096&quot;</td><td>29993372</td></tr><tr><td>&quot;id077&quot;</td><td>29990807</td></tr><tr><td>&quot;id074&quot;</td><td>30006309</td></tr><tr><td>&quot;id035&quot;</td><td>30003917</td></tr><tr><td>&quot;id081&quot;</td><td>29988686</td></tr><tr><td>&quot;id015&quot;</td><td>30006177</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;id039&quot;</td><td>30000168</td></tr><tr><td>&quot;id031&quot;</td><td>29998489</td></tr><tr><td>&quot;id063&quot;</td><td>30006173</td></tr><tr><td>&quot;id051&quot;</td><td>29994785</td></tr><tr><td>&quot;id089&quot;</td><td>29990077</td></tr><tr><td>&quot;id001&quot;</td><td>30009448</td></tr><tr><td>&quot;id012&quot;</td><td>29990141</td></tr><tr><td>&quot;id017&quot;</td><td>29995061</td></tr><tr><td>&quot;id062&quot;</td><td>29996661</td></tr><tr><td>&quot;id004&quot;</td><td>30015990</td></tr><tr><td>&quot;id058&quot;</td><td>29999957</td></tr><tr><td>&quot;id009&quot;</td><td>29994474</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (100, 2)\n",
       "┌───────┬──────────┐\n",
       "│ id1   ┆ v1_sum   │\n",
       "│ ---   ┆ ---      │\n",
       "│ str   ┆ i64      │\n",
       "╞═══════╪══════════╡\n",
       "│ id054 ┆ 30011978 │\n",
       "│ id055 ┆ 30009993 │\n",
       "│ id042 ┆ 29989540 │\n",
       "│ id084 ┆ 30005578 │\n",
       "│ …     ┆ …        │\n",
       "│ id062 ┆ 29996661 │\n",
       "│ id004 ┆ 30015990 │\n",
       "│ id058 ┆ 29999957 │\n",
       "│ id009 ┆ 29994474 │\n",
       "└───────┴──────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pl.scan_delta(f\"{Path.home()}/data/deltalake/G1_1e9_1e2_0_0\").group_by(\"id1\").agg(\n",
    "    pl.sum(\"v1\").alias(\"v1_sum\")\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7034451b-6ff2-4e3c-b6b7-6cfbafa0cd5f",
   "metadata": {},
   "source": [
    "## PySpark groupby query 3 (limit 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb6f5245-26aa-4059-8c39-bb114a2e1cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import delta\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from delta import configure_spark_with_delta_pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f39bcc6c-419c-44c0-9842-1f20d6626dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/matthew.powers/opt/miniconda3/envs/pyspark-340-delta-240/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/matthew.powers/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/matthew.powers/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-2ac595f3-8c72-4948-b048-c3f574f65c6d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 105ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-2ac595f3-8c72-4948-b048-c3f574f65c6d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/3ms)\n",
      "23/10/27 18:35:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "builder = (\n",
    "    pyspark.sql.SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.executor.memory\", \"10G\")\n",
    "    .config(\"spark.driver.memory\", \"25G\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5db184b0-652b-4b9b-ace7-db6ea926a879",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/27 18:35:37 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+------------------+\n",
      "|         id3| v1|                v3|\n",
      "+------------+---+------------------+\n",
      "|id0002654924|330|49.091885935185196|\n",
      "|id0001975574|329|48.816218894230765|\n",
      "|id0002470600|324|53.640876399999996|\n",
      "|id0001698780|285|45.204360193548375|\n",
      "|id0000058673|291|51.325111176470614|\n",
      "|id0003895227|358| 49.58640469565216|\n",
      "|id0005293826|295| 52.53704746315788|\n",
      "|id0007581630|300|48.697405603960384|\n",
      "|id0000651978|389| 49.91647398305082|\n",
      "|id0003454065|274|51.122333543478256|\n",
      "+------------+---+------------------+\n",
      "\n",
      "CPU times: user 111 ms, sys: 39.8 ms, total: 151 ms\n",
      "Wall time: 3min 54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark.read.format(\"delta\").load(f\"{Path.home()}/data/deltalake/G1_1e9_1e2_0_0\").createOrReplaceTempView(\"x\")\n",
    "spark.sql(\"select id3, sum(v1) as v1, mean(v3) as v3 from x group by id3 limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1687f842-5793-48e9-a451-8e4d78fab939",
   "metadata": {},
   "source": [
    "## PySpark groupby query 3 (limit 10) - different config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b818a099-00f9-49c4-a9c6-7d65fffc74f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import delta\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from delta import configure_spark_with_delta_pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc94fa4d-873e-4d3c-868e-ae9e37e54030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/matthew.powers/opt/miniconda3/envs/pyspark-340-delta-240/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/matthew.powers/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/matthew.powers/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-79526b79-83da-404f-bd8c-f36f6f38db99;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 109ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-79526b79-83da-404f-bd8c-f36f6f38db99\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/3ms)\n",
      "23/10/27 20:11:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "builder = (\n",
    "    pyspark.sql.SparkSession.builder.appName(\"MyApp\")\n",
    "    .master(\"local[1]\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.executor.memory\", \"10G\")\n",
    "    .config(\"spark.driver.memory\", \"25G\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "906f0f7c-51ea-40b2-9659-c5bb325f9cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/27 20:11:24 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+------------------+\n",
      "|         id3| v1|                v3|\n",
      "+------------+---+------------------+\n",
      "|id0002654924|330|49.091885935185196|\n",
      "|id0001975574|329|48.816218894230765|\n",
      "|id0002470600|324|53.640876399999996|\n",
      "|id0001698780|285|45.204360193548375|\n",
      "|id0000058673|291|51.325111176470614|\n",
      "|id0003895227|358| 49.58640469565216|\n",
      "|id0005293826|295| 52.53704746315788|\n",
      "|id0007581630|300|48.697405603960384|\n",
      "|id0000651978|389| 49.91647398305082|\n",
      "|id0003454065|274|51.122333543478256|\n",
      "+------------+---+------------------+\n",
      "\n",
      "CPU times: user 144 ms, sys: 62.2 ms, total: 206 ms\n",
      "Wall time: 10min 33s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark.read.format(\"delta\").load(f\"{Path.home()}/data/deltalake/G1_1e9_1e2_0_0\").createOrReplaceTempView(\"x\")\n",
    "spark.sql(\"select id3, sum(v1) as v1, mean(v3) as v3 from x group by id3 limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b0f3e3-7c2c-4093-b8fa-4d084c9044dd",
   "metadata": {},
   "source": [
    "## Polars groupby query 3 (limit 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5d642bb-9214-4a17-b4e2-c0eabb6b1122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41b673e4-8ceb-478b-bb3a-ad20b9aa3202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 33s, sys: 1min 28s, total: 5min 1s\n",
      "Wall time: 1min 41s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id3</th><th>v1_sum</th><th>v3_mean</th></tr><tr><td>str</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;id0007944062&quot;</td><td>344</td><td>44.816765</td></tr><tr><td>&quot;id0003196443&quot;</td><td>312</td><td>49.627266</td></tr><tr><td>&quot;id0004625609&quot;</td><td>273</td><td>46.772034</td></tr><tr><td>&quot;id0008566319&quot;</td><td>286</td><td>50.950365</td></tr><tr><td>&quot;id0006815127&quot;</td><td>259</td><td>52.738839</td></tr><tr><td>&quot;id0000694335&quot;</td><td>275</td><td>46.795619</td></tr><tr><td>&quot;id0001189103&quot;</td><td>265</td><td>48.9991</td></tr><tr><td>&quot;id0005475665&quot;</td><td>265</td><td>44.455095</td></tr><tr><td>&quot;id0006647807&quot;</td><td>318</td><td>51.353826</td></tr><tr><td>&quot;id0003601460&quot;</td><td>305</td><td>45.804697</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 3)\n",
       "┌──────────────┬────────┬───────────┐\n",
       "│ id3          ┆ v1_sum ┆ v3_mean   │\n",
       "│ ---          ┆ ---    ┆ ---       │\n",
       "│ str          ┆ i64    ┆ f64       │\n",
       "╞══════════════╪════════╪═══════════╡\n",
       "│ id0007944062 ┆ 344    ┆ 44.816765 │\n",
       "│ id0003196443 ┆ 312    ┆ 49.627266 │\n",
       "│ id0004625609 ┆ 273    ┆ 46.772034 │\n",
       "│ id0008566319 ┆ 286    ┆ 50.950365 │\n",
       "│ …            ┆ …      ┆ …         │\n",
       "│ id0001189103 ┆ 265    ┆ 48.9991   │\n",
       "│ id0005475665 ┆ 265    ┆ 44.455095 │\n",
       "│ id0006647807 ┆ 318    ┆ 51.353826 │\n",
       "│ id0003601460 ┆ 305    ┆ 45.804697 │\n",
       "└──────────────┴────────┴───────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "x = pl.scan_delta(f\"{Path.home()}/data/deltalake/G1_1e9_1e2_0_0\")\n",
    "x.group_by(\"id3\").agg([pl.sum(\"v1\").alias(\"v1_sum\"), pl.mean(\"v3\").alias(\"v3_mean\")]).limit(10).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39ea685-1488-4fec-a1ef-3319acb9dc5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-340-delta-240",
   "language": "python",
   "name": "pyspark-340-delta-240"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
