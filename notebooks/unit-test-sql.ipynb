{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdfbae7d-785a-49e6-ad76-7a2f678d3600",
   "metadata": {},
   "source": [
    "# Unit Test SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1746c77-854e-4430-b4c9-86e5b27a4b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pathlib\n",
    "\n",
    "import delta\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from delta import configure_spark_with_delta_pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "234c0ccb-85aa-4b4a-ad3c-66040c1eb510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/matthew.powers/opt/miniconda3/envs/pyspark-350-delta-310/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/matthew.powers/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/matthew.powers/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6683af6d-8889-4bd1-9903-6e7af5490286;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.1.0 in central\n",
      "\tfound io.delta#delta-storage;3.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 101ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6683af6d-8889-4bd1-9903-6e7af5490286\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/3ms)\n",
      "24/04/09 13:26:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "builder = (\n",
    "    pyspark.sql.SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.executor.memory\", \"10G\")\n",
    "    .config(\"spark.driver.memory\", \"25G\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f08f059a-d9de-491c-ad0f-4601dbf6cc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (\"socks\", 7.55, datetime.date(2022, 5, 15)),\n",
    "        (\"handbag\", 49.99, datetime.date(2022, 5, 16)),\n",
    "        (\"shorts\", 35.00, datetime.date(2023, 1, 5)),\n",
    "        (\"socks\", 25.00, datetime.date(2023, 12, 23)),\n",
    "    ],\n",
    "    [\"item\", \"amount\", \"purchase_date\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95c990b9-66dc-43d0-b9f6-f75e3a73eae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------------+\n",
      "|   item|amount|purchase_date|\n",
      "+-------+------+-------------+\n",
      "|  socks|  7.55|   2022-05-15|\n",
      "|handbag| 49.99|   2022-05-16|\n",
      "| shorts|  35.0|   2023-01-05|\n",
      "|  socks|  25.0|   2023-12-23|\n",
      "+-------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfe48c1d-7bbf-478c-ae0f-35cb264ea0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * from {df} where amount > {amount}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0e6495d-51a4-428f-8b6c-0430ee93ec90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------------+\n",
      "|   item|amount|purchase_date|\n",
      "+-------+------+-------------+\n",
      "|handbag| 49.99|   2022-05-16|\n",
      "| shorts|  35.0|   2023-01-05|\n",
      "+-------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query, df=df, amount=30.0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d2abd0a-4dbd-4578-bd50-1bd9c80146fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"my_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "340b8edd-0d47-4692-9bb3-eb94d041be31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------------+\n",
      "|   item|amount|purchase_date|\n",
      "+-------+------+-------------+\n",
      "|handbag| 49.99|   2022-05-16|\n",
      "| shorts|  35.0|   2023-01-05|\n",
      "+-------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"SELECT * from my_table where amount > 30.0\"\n",
    "\n",
    "spark.sql(\"SELECT * from my_table where amount > 30.0\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4dfc78a-0411-470d-a2a8-033f908586ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_df = spark.createDataFrame(\n",
    "    [\n",
    "        (\"handbag\", 49.99, datetime.date(2022, 5, 16)),\n",
    "        (\"shorts\", 35.00, datetime.date(2023, 1, 5)),\n",
    "    ],\n",
    "    [\"item\", \"amount\", \"purchase_date\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7582099-658b-4224-8e50-7a439b87d10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.testing import assertDataFrameEqual\n",
    "actual_df = spark.sql(query, df=df, amount=30.0)\n",
    "assertDataFrameEqual(actual_df, expected_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbd4737-2c1c-461c-8bd2-dfda735d11f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-350-delta-310",
   "language": "python",
   "name": "pyspark-350-delta-310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
