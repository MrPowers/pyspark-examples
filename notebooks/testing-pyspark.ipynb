{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "918defe9-0abb-459d-8cda-0bef3fe06ea4",
   "metadata": {},
   "source": [
    "# Testing PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d5899c-18e9-4d16-9ac7-4b0d260c4066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01450dbb-874b-4083-a3bb-d2443453e2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/04 08:12:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a9d396c-d32d-4bec-95c6-97cd0434a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame(data=[(\"1\", 1000), (\"2\", 3200)], schema=[\"id\", \"amount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc2828bb-25a2-4436-9096-cab88c725d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|amount|\n",
      "+---+------+\n",
      "|  1|  1000|\n",
      "|  2|  3200|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f09f586-d1aa-4615-9f20-59ce975cb493",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame(data=[(\"1\", 1000), (\"2\", 3000)], schema=[\"id\", \"amount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "066d31f4-a6d4-470e-8d42-d12021580850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|amount|\n",
      "+---+------+\n",
      "|  1|  1000|\n",
      "|  2|  3000|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d55e9ecf-1f3b-4d54-8ffb-8f8ad9b72db1",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkAssertionError",
     "evalue": "[DIFFERENT_ROWS] Results do not match: ( 50.00000 % )\n*** actual ***\n  Row(id='1', amount=1000)\n\u001b[31m! Row(id='2', amount=3200)\u001b[0m\n\n\n*** expected ***\n  Row(id='1', amount=1000)\n\u001b[31m! Row(id='2', amount=3000)\u001b[0m",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkAssertionError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43massertDataFrameEqual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf2\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pass, DataFrames are identical\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/pyspark-350/lib/python3.9/site-packages/pyspark/testing/utils.py:604\u001b[0m, in \u001b[0;36massertDataFrameEqual\u001b[0;34m(actual, expected, checkRowOrder, rtol, atol)\u001b[0m\n\u001b[1;32m    601\u001b[0m     actual_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(actual_list, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mstr\u001b[39m(x))\n\u001b[1;32m    602\u001b[0m     expected_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(expected_list, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mstr\u001b[39m(x))\n\u001b[0;32m--> 604\u001b[0m \u001b[43massert_rows_equal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactual_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/pyspark-350/lib/python3.9/site-packages/pyspark/testing/utils.py:579\u001b[0m, in \u001b[0;36massertDataFrameEqual.<locals>.assert_rows_equal\u001b[0;34m(rows1, rows2)\u001b[0m\n\u001b[1;32m    577\u001b[0m error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m( \u001b[39m\u001b[38;5;132;01m%.5f\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m )\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m percent_diff\n\u001b[1;32m    578\u001b[0m error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(generated_diff)\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m PySparkAssertionError(\n\u001b[1;32m    580\u001b[0m     error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDIFFERENT_ROWS\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    581\u001b[0m     message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror_msg\u001b[39m\u001b[38;5;124m\"\u001b[39m: error_msg},\n\u001b[1;32m    582\u001b[0m )\n",
      "\u001b[0;31mPySparkAssertionError\u001b[0m: [DIFFERENT_ROWS] Results do not match: ( 50.00000 % )\n*** actual ***\n  Row(id='1', amount=1000)\n\u001b[31m! Row(id='2', amount=3200)\u001b[0m\n\n\n*** expected ***\n  Row(id='1', amount=1000)\n\u001b[31m! Row(id='2', amount=3000)\u001b[0m"
     ]
    }
   ],
   "source": [
    "from pyspark.testing import assertDataFrameEqual\n",
    "assertDataFrameEqual(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ea088b-d31b-45ef-a944-e6906e9f1a48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55922fa0-ddf6-44c0-a054-50b98965de91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-350",
   "language": "python",
   "name": "pyspark-350"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
